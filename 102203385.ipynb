{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226153cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb046bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.clustering import *\n",
    "\n",
    "s=setup(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee4ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['DEATH_EVENT'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa97962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.clustering import *\n",
    "\n",
    "s=setup(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "#Define preprocessing variants\n",
    "preprocessing_variants = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "#Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'GaussianMixture': 'gmm'\n",
    "}\n",
    "\n",
    "#Number of clusters to evaluate\n",
    "clusters_to_test = [3, 4, 5]\n",
    "\n",
    "#Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocessing_variants.items():\n",
    "        for k in clusters_to_test:\n",
    "            print(f'\\n Model: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      session_id=123,\n",
    "                      silent=True,\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f' Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "#Define preprocessing variants\n",
    "preprocessing_variants = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "#Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'GaussianMixture': 'gmm'\n",
    "}\n",
    "\n",
    "#Number of clusters to evaluate\n",
    "clusters_to_test = [3, 4, 5]\n",
    "\n",
    "#Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocessing_variants.items():\n",
    "        for k in clusters_to_test:\n",
    "            print(f'\\n Model: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f' Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "#Define preprocessing variants\n",
    "preprocessing_variants = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "#Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'KMedoids': 'kmedoids'\n",
    "}\n",
    "\n",
    "#Number of clusters to evaluate\n",
    "clusters_to_test = [3, 4, 5]\n",
    "\n",
    "#Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocessing_variants.items():\n",
    "        for k in clusters_to_test:\n",
    "            print(f'\\n Model: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f' Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec0a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "#Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "#Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "#Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "#Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\n Model: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f' Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\n Model: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=False,\n",
    "                      session_id=123,\n",
    "                      silent=True)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f' Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\n📄 Results saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b905cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\n Model: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f' Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\n📄 Results saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c63def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Filter out rows with errors\n",
    "clean_df = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "]\n",
    "\n",
    "# Convert scores to float\n",
    "for col in ['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']:\n",
    "    clean_df[col] = clean_df[col].astype(float)\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "\n",
    "# Plot 1: Silhouette Score\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=clean_df, x='Clusters', y='Silhouette Score', hue='Model')\n",
    "plt.title('Silhouette Score by Model and Number of Clusters')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Calinski-Harabasz Index\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=clean_df, x='Clusters', y='Calinski-Harabasz Index', hue='Model')\n",
    "plt.title('Calinski-Harabasz Index by Model and Number of Clusters')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Davies-Bouldin Index (lower is better)\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=clean_df, x='Clusters', y='Davies-Bouldin Index', hue='Model')\n",
    "plt.title('Davies-Bouldin Index by Model and Number of Clusters (Lower is Better)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda1c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results CSV\n",
    "results_df = pd.read_csv('/path/to/your/clustering_results.csv')\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot 1: Silhouette Score Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Clusters', y='Silhouette Score', hue='Model')\n",
    "plt.title('Silhouette Score by Model and Cluster Count')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Calinski-Harabasz Index Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Clusters', y='Calinski-Harabasz Index', hue='Model')\n",
    "plt.title('Calinski-Harabasz Index by Model and Cluster Count')\n",
    "plt.ylabel('CH Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Davies-Bouldin Index Comparison (lower is better)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Clusters', y='Davies-Bouldin Index', hue='Model')\n",
    "plt.title('Davies-Bouldin Index by Model and Cluster Count')\n",
    "plt.ylabel('DB Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d987c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot 1: Silhouette Score Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Clusters', y='Silhouette Score', hue='Model')\n",
    "plt.title('Silhouette Score by Model and Cluster Count')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Calinski-Harabasz Index Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Clusters', y='Calinski-Harabasz Index', hue='Model')\n",
    "plt.title('Calinski-Harabasz Index by Model and Cluster Count')\n",
    "plt.ylabel('CH Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Davies-Bouldin Index Comparison (lower is better)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=results_df, x='Clusters', y='Davies-Bouldin Index', hue='Model')\n",
    "plt.title('Davies-Bouldin Index by Model and Cluster Count')\n",
    "plt.ylabel('DB Index')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Set up Seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Bar Plot for Silhouette Score comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Silhouette Score', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Silhouette Score')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar Plot for Calinski-Harabasz Index comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Calinski-Harabasz Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Calinski-Harabasz Index')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar Plot for Davies-Bouldin Index comparison (lower is better)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Davies-Bouldin Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Davies-Bouldin Index')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Set up Seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Bar Plot for Silhouette Score comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Silhouette Score', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Silhouette Score')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Clustering Model')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar Plot for Calinski-Harabasz Index comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Calinski-Harabasz Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Calinski-Harabasz Index')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar Plot for Davies-Bouldin Index comparison (lower is better)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Davies-Bouldin Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Davies-Bouldin Index')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e04e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Set up Seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Bar Plot for Silhouette Score comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Silhouette Score', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Silhouette Score')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar Plot for Calinski-Harabasz Index comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Calinski-Harabasz Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Calinski-Harabasz Index')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar Plot for Davies-Bouldin Index comparison (lower is better)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Davies-Bouldin Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Davies-Bouldin Index')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Heatmap of Evaluation Metrics for each Model, Preprocessing, and Number of Clusters\n",
    "pivot_df = results_df.pivot_table(index=['Model', 'Preprocessing'], columns='Clusters', values=['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index'])\n",
    "\n",
    "# Plotting Heatmap for Silhouette Score\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_df['Silhouette Score'], annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Heatmap of Silhouette Score across Models, Preprocessing, and Number of Clusters')\n",
    "plt.ylabel('Model & Preprocessing')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Heatmap for Calinski-Harabasz Index\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_df['Calinski-Harabasz Index'], annot=True, cmap='coolwarm', fmt='.1f', linewidths=0.5)\n",
    "plt.title('Heatmap of Calinski-Harabasz Index across Models, Preprocessing, and Number of Clusters')\n",
    "plt.ylabel('Model & Preprocessing')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Heatmap for Davies-Bouldin Index\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_df['Davies-Bouldin Index'], annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Heatmap of Davies-Bouldin Index across Models, Preprocessing, and Number of Clusters')\n",
    "plt.ylabel('Model & Preprocessing')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08184ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"✅ Best Preprocessing + Cluster Count for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\n🏆 Final Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63857b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\" Best Preprocessing + Cluster Count for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\n Final Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61854fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\nModel: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Preprocessing + Cluster Count for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb50e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cf654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyCaret without preprocessing\n",
    "s = setup(data=df, normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Create KMeans with 5 clusters\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign clusters\n",
    "labeled_df = assign_model(kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b4eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df, normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544da03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "# Setup with no preprocessing\n",
    "s = setup(data=df, normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccba697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12128a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51478edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5552a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\nModel: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      verbose=True)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Set up Seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Bar Plot for Silhouette Score comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Silhouette Score', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Silhouette Score')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar Plot for Calinski-Harabasz Index comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Calinski-Harabasz Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Calinski-Harabasz Index')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar Plot for Davies-Bouldin Index comparison (lower is better)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Davies-Bouldin Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Davies-Bouldin Index')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ac5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af666025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=True)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46613f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\nModel: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      session_id=123,\n",
    "                      verbose=True)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acc682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=True, session_id=123)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e28b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\nModel: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      session_id=123,\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565021f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37546587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\nModel: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      # session_id=123,\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3911659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab9533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from pycaret.clustering import *\n",
    "import pandas as pd\n",
    "\n",
    "# Define preprocessing variants\n",
    "preprocess_options = {\n",
    "    'None': {'normalize': False, 'transformation': False, 'pca': False},\n",
    "    'Normalization': {'normalize': True, 'transformation': False, 'pca': False},\n",
    "    'Transformation': {'normalize': False, 'transformation': True, 'pca': False},\n",
    "    'PCA': {'normalize': False, 'transformation': False, 'pca': True},\n",
    "    'T + N': {'normalize': True, 'transformation': True, 'pca': False},\n",
    "    'T + N + PCA': {'normalize': True, 'transformation': True, 'pca': True},\n",
    "}\n",
    "\n",
    "# Define models and clustering methods\n",
    "models = {\n",
    "    'KMeans': 'kmeans',\n",
    "    'Hierarchical': 'hclust',\n",
    "    'Spectral': 'sc'\n",
    "}\n",
    "\n",
    "# Number of clusters to evaluate\n",
    "cluster_counts = [3, 4, 5]\n",
    "\n",
    "# Store results in a list of dicts\n",
    "results = []\n",
    "\n",
    "# Begin model-preprocessing-cluster loop\n",
    "for model_name, model_id in models.items():\n",
    "    for prep_name, prep in preprocess_options.items():\n",
    "        for k in cluster_counts:\n",
    "            print(f'\\nModel: {model_name} | Preprocessing: {prep_name} | Clusters: {k}')\n",
    "            try:\n",
    "                # Setup PyCaret environment\n",
    "                setup(data=df.copy(),\n",
    "                      normalize=prep['normalize'],\n",
    "                      transformation=prep['transformation'],\n",
    "                      pca=prep['pca'],\n",
    "                      # session_id=123,\n",
    "                      verbose=False)\n",
    "\n",
    "                # Train the model\n",
    "                model = create_model(model_id, num_clusters=k)\n",
    "\n",
    "                # Assign clusters to dataset\n",
    "                clustered_df = assign_model(model)\n",
    "\n",
    "                # Get cluster labels and features\n",
    "                labels = clustered_df['Cluster']\n",
    "                features = get_config('X')\n",
    "\n",
    "                # Evaluate metrics\n",
    "                sil = silhouette_score(features, labels)\n",
    "                ch = calinski_harabasz_score(features, labels)\n",
    "                db = davies_bouldin_score(features, labels)\n",
    "\n",
    "                # Save result\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': round(sil, 3),\n",
    "                    'Calinski-Harabasz Index': round(ch, 1),\n",
    "                    'Davies-Bouldin Index': round(db, 3)\n",
    "                })\n",
    "\n",
    "                # Print results\n",
    "                print(f'Silhouette Score: {sil:.3f}')\n",
    "                print(f'Calinski-Harabasz Index: {ch:.1f}')\n",
    "                print(f'Davies-Bouldin Index: {db:.3f}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error: {e}')\n",
    "                results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Preprocessing': prep_name,\n",
    "                    'Clusters': k,\n",
    "                    'Silhouette Score': 'Error',\n",
    "                    'Calinski-Harabasz Index': 'Error',\n",
    "                    'Davies-Bouldin Index': 'Error'\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame and export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'clustering_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624771f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the results CSV\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Remove 'Error' entries\n",
    "results_df_clean = results_df[\n",
    "    (results_df['Silhouette Score'] != 'Error') &\n",
    "    (results_df['Calinski-Harabasz Index'] != 'Error') &\n",
    "    (results_df['Davies-Bouldin Index'] != 'Error')\n",
    "].copy()\n",
    "\n",
    "# Convert to proper types\n",
    "results_df_clean['Silhouette Score'] = results_df_clean['Silhouette Score'].astype(float)\n",
    "results_df_clean['Calinski-Harabasz Index'] = results_df_clean['Calinski-Harabasz Index'].astype(float)\n",
    "results_df_clean['Davies-Bouldin Index'] = results_df_clean['Davies-Bouldin Index'].astype(float)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(results_df_clean[['Silhouette Score', 'Calinski-Harabasz Index', 'Davies-Bouldin Index']])\n",
    "\n",
    "# Invert Davies-Bouldin (lower is better)\n",
    "scaled[:, 2] = 1 - scaled[:, 2]\n",
    "\n",
    "# Add composite score\n",
    "results_df_clean['Composite Score'] = (\n",
    "    0.4 * scaled[:, 0] + \n",
    "    0.4 * scaled[:, 1] + \n",
    "    0.2 * scaled[:, 2]\n",
    ")\n",
    "\n",
    "# Step 1: Best config for each model\n",
    "best_per_model = results_df_clean.loc[results_df_clean.groupby('Model')['Composite Score'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "print(\"Best Configuration for Each Model:\\n\")\n",
    "print(best_per_model[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                      'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])\n",
    "\n",
    "# Step 2: Overall best config\n",
    "overall_best = best_per_model.loc[best_per_model['Composite Score'].idxmax()]\n",
    "print(\"\\nFinal Best Configuration Across All Models:\\n\")\n",
    "print(overall_best[['Model', 'Preprocessing', 'Clusters', 'Silhouette Score', \n",
    "                    'Calinski-Harabasz Index', 'Davies-Bouldin Index', 'Composite Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38300d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=False, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d739852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with no preprocessing\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=True, verbose=False)\n",
    "\n",
    "# Train model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign model\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get features and labels\n",
    "X = get_config('X')\n",
    "labels = labeled_df['Cluster']\n",
    "\n",
    "# Manually compute metrics\n",
    "sil = silhouette_score(X, labels)\n",
    "ch = calinski_harabasz_score(X, labels)\n",
    "db = davies_bouldin_score(X, labels)\n",
    "\n",
    "print(f\"Silhouette Score: {sil:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff99ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup with PCA enabled and fixed seed\n",
    "from pycaret.clustering import *\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
    "df.drop(columns=['DEATH_EVENT'], inplace=True)\n",
    "\n",
    "# Setup with PCA\n",
    "s = setup(data=df.copy(), normalize=False, transformation=False, pca=True, session_id=123, verbose=False)\n",
    "\n",
    "# Train KMeans model\n",
    "kmeans_model = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "# Assign clusters\n",
    "labeled_df = assign_model(kmeans_model)\n",
    "\n",
    "# Get PCA components and cluster labels\n",
    "pca_features = get_config('X')  # Already in PCA form\n",
    "pca_df = pd.DataFrame(pca_features, columns=['PC1', 'PC2'])  # Keep only 2 for plotting\n",
    "pca_df['Cluster'] = labeled_df['Cluster']\n",
    "\n",
    "# Plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=80)\n",
    "plt.title('KMeans Clustering with PCA (5 Clusters)', fontsize=16)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23824cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(kmeans_model, plot='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(kmeans_model, plot = 'tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b235aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "results_df = pd.read_csv('clustering_results.csv')\n",
    "\n",
    "# Set up Seaborn for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1. Bar Plot for Silhouette Score comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Silhouette Score', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Silhouette Score')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar Plot for Calinski-Harabasz Index comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Calinski-Harabasz Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Calinski-Harabasz Index')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Bar Plot for Davies-Bouldin Index comparison (lower is better)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df, x='Model', y='Davies-Bouldin Index', hue='Preprocessing', ci=None)\n",
    "plt.title('Comparison of Clustering Models based on Davies-Bouldin Index')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.xlabel('Clustering Model')\n",
    "plt.legend(title='Preprocessing', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = '/content/102203385.ipynb'\n",
    "\n",
    "# Load\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Clean widget metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Save it back\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Cleaned notebook metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea894cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = '102203385.ipynb'\n",
    "\n",
    "# Load\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Clean widget metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Save it back\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Cleaned notebook metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import nbformat\n",
    "\n",
    "notebook_name = 'Clustering-Assignment.ipynb'\n",
    "\n",
    "# Save the current notebook\n",
    "!jupyter nbconvert --to notebook --output=\"{notebook_name}\" --inplace \"/content/{notebook_name}\"\n",
    "print(f\"✅ Saved notebook as {notebook_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import nbformat\n",
    "\n",
    "notebook_name = '102203385.ipynb'\n",
    "\n",
    "# Save the current notebook\n",
    "!jupyter nbconvert --to notebook --output=\"{notebook_name}\" --inplace \"/content/{notebook_name}\"\n",
    "print(f\"✅ Saved notebook as {notebook_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = '102203385.ipynb'\n",
    "\n",
    "# Load\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Clean widget metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Save it back\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Cleaned notebook metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = '102203385.ipynb'\n",
    "\n",
    "# Load\n",
    "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Clean widget metadata\n",
    "if 'widgets' in nb.metadata:\n",
    "    del nb.metadata['widgets']\n",
    "\n",
    "# Save it back\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Cleaned notebook metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import nbformat\n",
    "\n",
    "notebook_name = 'Clustering-Assignment.ipynb'\n",
    "\n",
    "# Save the current notebook\n",
    "!jupyter nbconvert --to notebook --output=\"{notebook_name}\" --inplace \"/content/{notebook_name}\"\n",
    "print(f\"✅ Saved notebook as {notebook_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import nbformat\n",
    "\n",
    "notebook_name = '102203385.ipynb'\n",
    "\n",
    "# Save the current notebook\n",
    "!jupyter nbconvert --to notebook --output=\"{notebook_name}\" --inplace \"/content/{notebook_name}\"\n",
    "print(f\"✅ Saved notebook as {notebook_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import nbformat\n",
    "\n",
    "notebook_name = '102203385.ipynb'\n",
    "\n",
    "# Save the current notebook\n",
    "!jupyter nbconvert --to notebook --output=\"{notebook_name}\" --inplace \"{notebook_name}\"\n",
    "print(f\"✅ Saved notebook as {notebook_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "import nbformat\n",
    "\n",
    "# Step 1: Get the notebook content\n",
    "ipython = get_ipython()\n",
    "notebook_content = ipython.history_manager.input_hist_raw\n",
    "\n",
    "# Step 2: Create a new notebook object\n",
    "nb = nbformat.v4.new_notebook()\n",
    "nb.cells = [nbformat.v4.new_code_cell(code) for code in notebook_content]\n",
    "\n",
    "# Optional: Clean up any metadata (just to be safe)\n",
    "nb.metadata.pop('widgets', None)\n",
    "\n",
    "# Step 3: Save cleaned notebook\n",
    "cleaned_notebook_path = '/content/102203385.ipynb'\n",
    "with open(cleaned_notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"✅ Notebook cleaned and saved as 102203385.ipynb\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
